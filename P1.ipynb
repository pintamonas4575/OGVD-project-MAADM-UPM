{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/25 17:23:54 WARN Utils: Your hostname, jalvarez resolves to a loopback address: 127.0.1.1; using 10.166.129.52 instead (on interface wlo1)\n",
      "25/02/25 17:23:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/25 17:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import findspark\n",
    "findspark.init()\n",
    "import random\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "import numpy as np\n",
    "sc = SparkContext(\"local[*]\", \"Name of the Program\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    rdd = sc.textFile(filename)\n",
    "    rdd_numpy = rdd.map(lambda x : np.array(x.split(\",\"), dtype = float))\n",
    "    rdd_return = rdd_numpy.map(lambda x : (x[:-1], int(x[-1])))\n",
    "    return rdd_return\n",
    "\n",
    "def normalize(RDD_Xy):\n",
    "    \n",
    "    suma = RDD_Xy.reduce(lambda x,y: (np.array(x[0])+np.array(y[0]),x[1]))\n",
    "    total_filas = float(RDD_Xy.count())\n",
    "    media = suma[0] / total_filas\n",
    "\n",
    "    rdd_2 = RDD_Xy.map(lambda x: (x[0],x[1], media)) \n",
    "\n",
    "    restas = rdd_2.map(lambda x: (x[0]-x[2])**2)\n",
    "    sumas = restas.reduce(lambda x,y: x+y)\n",
    "    desviacion = (sumas / total_filas)**0.5\n",
    "\n",
    "    def f(x):\n",
    "        a = (x[0] - media) / desviacion\n",
    "        a= np.nan_to_num(a, copy=False,nan=0.0)\n",
    "        return (a,x[1])\n",
    "\n",
    "    normalizado = RDD_Xy.map(f)\n",
    "    return normalizado\n",
    "\n",
    "def predict(w,b,X,threshold=0.5):\n",
    "    rdd=X.map(lambda x:np.dot(w,np.array(x[0])+b))\n",
    "    logistic= rdd.map(lambda x: 1/(1+np.exp(-x)))\n",
    "    results=logistic.map(lambda x: 1 if x>threshold else 0)\n",
    "    return results\n",
    "\n",
    "def get_predictions_targets_inputs(w,b,X,threshold=0.5):\n",
    "    \"\"\"\n",
    "    Get the predictions, targets and inputs\n",
    "    :param w: weights\n",
    "    :param b: bias\n",
    "    :param X: RDD with the data\n",
    "    :param threshold: threshold to classify the data\n",
    "    :return: RDD with the tuple predictions, targets and inputs\n",
    "    \"\"\"\n",
    "    rdd=X.map(lambda x:(np.dot(w,np.array(x[0])+b),x[1],x[0]))\n",
    "    logistic= rdd.map(lambda x: (1/(1+np.exp(-x[0])),x[1],x[2]))\n",
    "    results=logistic.map(lambda x: ((0.999,x[1],x[2]) if x[0]>threshold else (0.001,x[1],x[2]))) #para evitar log(0)=-inf y log(1)=0\n",
    "    return results\n",
    "\n",
    "# TODO preguntar si la b se incluye en la perdida, porque en la formula del enunciado no aparece\n",
    "\n",
    "def calculate_loss_function(predictions_and_targets,lambda_reg,w):   \n",
    "    \"\"\"\n",
    "    Calculate the loss function for logistic regression\n",
    "    :param predictions_and_targets: RDD with the predictions and the targets\n",
    "    :param lambda_reg: regularization parameter\n",
    "    :param w: weights\n",
    "    :return: loss function\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        return (-x[1]*np.log(x[0])-(1-x[1])*np.log(1-x[0]))\n",
    "\n",
    "    loss=predictions_and_targets.map(f)\n",
    "    suma=loss.reduce(lambda x,y: x+y)/predictions_and_targets.count()\n",
    "    squared_w=np.dot(w,w)\n",
    "    squared_w=squared_w*lambda_reg/(2*len(w))\n",
    "    return suma+squared_w\n",
    "\n",
    "\n",
    "\n",
    "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
    "    w = np.random.rand(11)\n",
    "    b = np.random.rand(1)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        predictions_targets_inputs = get_predictions_targets_inputs(w, b, RDD_Xy)\n",
    "        \n",
    "        count = predictions_targets_inputs.count()\n",
    "        \n",
    "        gradients = predictions_targets_inputs.map(lambda x: ((x[0] - x[1]) * np.array(x[2]), x[0] - x[1]))\n",
    "        \n",
    "        gradient_w = gradients.map(lambda x: x[0]).reduce(lambda x, y: x + y) / count + (lambda_reg * w) / len(w)\n",
    "        gradient_b = gradients.map(lambda x: x[1]).reduce(lambda x, y: x + y) / count\n",
    "        \n",
    "        w -= learning_rate * gradient_w\n",
    "        b -= learning_rate * gradient_b\n",
    "        \n",
    "        loss = calculate_loss_function(predictions_targets_inputs, lambda_reg, w)\n",
    "        print(f\"Iteration: {iteration}, Loss: {loss}\")\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def accuracy(w,b,RDD_Xy):\n",
    "    predictions_and_targets=get_predictions_targets_inputs(w,b,RDD_Xy)\n",
    "    correct=predictions_and_targets.map(lambda x: 1 if round(x[0])==x[1] else 0)\n",
    "    correct=correct.reduce(lambda x,y: x+y)\n",
    "    return correct/predictions_and_targets.count()\n",
    "\n",
    "def transform(data,num_blocks_cv):\n",
    "    data = data.map(lambda x: (random.randint(0, num_blocks_cv-1), x[0],x[1]))\n",
    "    return data\n",
    "\n",
    "def get_block_data(data_cv: pyspark.rdd.RDD, i):\n",
    "    train_data = data_cv.flatMap(lambda x: [(x[1],x[2])] if x[0] != i else [])\n",
    "    test_data = data_cv.flatMap(lambda x: [(x[1],x[2])] if x[0] == i else [])\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIO 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 1.9245317061870915\n",
      "Iteration: 1, Loss: 0.9776156260344829\n",
      "Iteration: 2, Loss: 0.7289724540031696\n",
      "Iteration: 3, Loss: 0.5956720867752253\n",
      "Iteration: 4, Loss: 0.552159531669732\n",
      "Iteration: 5, Loss: 0.5190071087322141\n",
      "Iteration: 6, Loss: 0.502430897263453\n",
      "Iteration: 7, Loss: 0.48378265936109227\n",
      "Iteration: 8, Loss: 0.4699691498037915\n",
      "Iteration: 9, Loss: 0.457536991202221\n",
      "Final accuracy: 0.9339\n"
     ]
    }
   ],
   "source": [
    "n = readFile('data/botnet_reduced_10k_l.csv')\n",
    "\n",
    "normalizado = normalize(n)\n",
    "\n",
    "w,b=train(normalizado,10,1.5,0)\n",
    "\n",
    "acc=accuracy(w,b,normalizado)\n",
    "print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EJERCICIO 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Block 0\n",
      "Iteration: 0, Loss: 3.0868364437872655\n",
      "Iteration: 1, Loss: 1.1129823937660963\n",
      "Iteration: 2, Loss: 0.7527471352878818\n",
      "Iteration: 3, Loss: 0.6149009032459319\n",
      "Iteration: 4, Loss: 0.5342353202655583\n",
      "Iteration: 5, Loss: 0.5079034559835538\n",
      "Iteration: 6, Loss: 0.5007452693578025\n",
      "Iteration: 7, Loss: 0.4703498005937092\n",
      "Iteration: 8, Loss: 0.4554730462987981\n",
      "Iteration: 9, Loss: 0.4595030906550111\n",
      "Accuracy: 0.8932135728542914\n",
      "--------------------------------------------------\n",
      "Block 1\n",
      "Iteration: 0, Loss: 1.5969845082549854\n",
      "Iteration: 1, Loss: 0.8692578743817179\n",
      "Iteration: 2, Loss: 0.6770508786222551\n",
      "Iteration: 3, Loss: 0.5689501087559797\n",
      "Iteration: 4, Loss: 0.5294071657633834\n",
      "Iteration: 5, Loss: 0.5053787031931468\n",
      "Iteration: 6, Loss: 0.4906863702818644\n",
      "Iteration: 7, Loss: 0.4748132204333365\n",
      "Iteration: 8, Loss: 0.4632403599925558\n",
      "Iteration: 9, Loss: 0.4647525587993206\n",
      "Accuracy: 0.9104477611940298\n",
      "--------------------------------------------------\n",
      "Block 2\n",
      "Iteration: 0, Loss: 2.9686966610199383\n",
      "Iteration: 1, Loss: 1.5622883013474027\n",
      "Iteration: 2, Loss: 0.9957699681206327\n",
      "Iteration: 3, Loss: 0.75475248502122\n",
      "Iteration: 4, Loss: 0.6488000433253607\n",
      "Iteration: 5, Loss: 0.5690059292143775\n",
      "Iteration: 6, Loss: 0.5281201755088911\n",
      "Iteration: 7, Loss: 0.5109898312009693\n",
      "Iteration: 8, Loss: 0.503644089489262\n",
      "Iteration: 9, Loss: 0.4989577480252096\n",
      "Accuracy: 0.9238476953907816\n",
      "--------------------------------------------------\n",
      "Block 3\n",
      "Iteration: 0, Loss: 3.181406757271161\n",
      "Iteration: 1, Loss: 1.8023974490530452\n",
      "Iteration: 2, Loss: 0.939799779151172\n",
      "Iteration: 3, Loss: 0.7020413220201429\n",
      "Iteration: 4, Loss: 0.6221774914617139\n",
      "Iteration: 5, Loss: 0.5548829150132544\n",
      "Iteration: 6, Loss: 0.5439295244067133\n",
      "Iteration: 7, Loss: 0.5124749939101271\n",
      "Iteration: 8, Loss: 0.4861453648554679\n",
      "Iteration: 9, Loss: 0.4866821988013608\n",
      "Accuracy: 0.9430255402750491\n",
      "--------------------------------------------------\n",
      "Block 4\n",
      "Iteration: 0, Loss: 3.1344853904906564\n",
      "Iteration: 1, Loss: 1.2444653772056882\n",
      "Iteration: 2, Loss: 0.8200516977668431\n",
      "Iteration: 3, Loss: 0.6651056422870504\n",
      "Iteration: 4, Loss: 0.5780892392524511\n",
      "Iteration: 5, Loss: 0.5511204953128294\n",
      "Iteration: 6, Loss: 0.5015892730341946\n",
      "Iteration: 7, Loss: 0.4914321707046236\n",
      "Iteration: 8, Loss: 0.47275763532642523\n",
      "Iteration: 9, Loss: 0.4593053744225102\n",
      "Accuracy: 0.9453441295546559\n",
      "--------------------------------------------------\n",
      "Block 5\n",
      "Iteration: 0, Loss: 1.8545363534511765\n",
      "Iteration: 1, Loss: 1.1854759287399217\n",
      "Iteration: 2, Loss: 0.9264651914355193\n",
      "Iteration: 3, Loss: 0.7478462197788066\n",
      "Iteration: 4, Loss: 0.6702049208714472\n",
      "Iteration: 5, Loss: 0.591845092607363\n",
      "Iteration: 6, Loss: 0.5423282501324371\n",
      "Iteration: 7, Loss: 0.5259845171352437\n",
      "Iteration: 8, Loss: 0.5022645339359882\n",
      "Iteration: 9, Loss: 0.47266933000757344\n",
      "Accuracy: 0.9273797841020608\n",
      "--------------------------------------------------\n",
      "Block 6\n",
      "Iteration: 0, Loss: 1.6748082497863903\n",
      "Iteration: 1, Loss: 1.0337168796155554\n",
      "Iteration: 2, Loss: 0.7166786645347428\n",
      "Iteration: 3, Loss: 0.6138088949884691\n",
      "Iteration: 4, Loss: 0.5171285341985135\n",
      "Iteration: 5, Loss: 0.48480869930349\n",
      "Iteration: 6, Loss: 0.4770919499983664\n",
      "Iteration: 7, Loss: 0.454039797800642\n",
      "Iteration: 8, Loss: 0.46423299303390414\n",
      "Iteration: 9, Loss: 0.45519302534609263\n",
      "Accuracy: 0.9611451942740287\n",
      "--------------------------------------------------\n",
      "Block 7\n",
      "Iteration: 0, Loss: 3.509639487222821\n",
      "Iteration: 1, Loss: 1.721145523482857\n",
      "Iteration: 2, Loss: 1.2207785957663904\n",
      "Iteration: 3, Loss: 0.7763333712142566\n",
      "Iteration: 4, Loss: 0.6762373437256292\n",
      "Iteration: 5, Loss: 0.6230182113987159\n",
      "Iteration: 6, Loss: 0.5842752001254792\n",
      "Iteration: 7, Loss: 0.5573836381154806\n",
      "Iteration: 8, Loss: 0.5435824388259287\n",
      "Iteration: 9, Loss: 0.5216075680159544\n",
      "Accuracy: 0.905337361530715\n",
      "--------------------------------------------------\n",
      "Block 8\n",
      "Iteration: 0, Loss: 2.561063596044581\n",
      "Iteration: 1, Loss: 1.3258124313146185\n",
      "Iteration: 2, Loss: 0.7935757037197665\n",
      "Iteration: 3, Loss: 0.6345235940222599\n",
      "Iteration: 4, Loss: 0.549294113937063\n",
      "Iteration: 5, Loss: 0.5098923827030816\n",
      "Iteration: 6, Loss: 0.49197738643721806\n",
      "Iteration: 7, Loss: 0.4702044325385577\n",
      "Iteration: 8, Loss: 0.47659565345170257\n",
      "Iteration: 9, Loss: 0.4446109771961861\n",
      "Accuracy: 0.945054945054945\n",
      "--------------------------------------------------\n",
      "Block 9\n",
      "Iteration: 0, Loss: 3.1568868990818024\n",
      "Iteration: 1, Loss: 1.2917624719835243\n",
      "Iteration: 2, Loss: 0.890213544753561\n",
      "Iteration: 3, Loss: 0.762707422727941\n",
      "Iteration: 4, Loss: 0.657657707960759\n",
      "Iteration: 5, Loss: 0.5959804822619293\n",
      "Iteration: 6, Loss: 0.5712001854555385\n",
      "Iteration: 7, Loss: 0.5303346301698794\n",
      "Iteration: 8, Loss: 0.514064787023377\n",
      "Iteration: 9, Loss: 0.48717312769560567\n",
      "Accuracy: 0.926605504587156\n"
     ]
    }
   ],
   "source": [
    "num_blocks_cv=10\n",
    "rdd_with_keys = transform(normalizado, num_blocks_cv)\n",
    "all_ac=[]\n",
    "for i in range(num_blocks_cv):\n",
    "    train_data, test_data = get_block_data(rdd_with_keys, i)\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Block {i}\")\n",
    "    w,b=train(train_data,10,1.5,0)\n",
    "    acc=accuracy(w,b,test_data)\n",
    "    all_ac.append(acc)\n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
